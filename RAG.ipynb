{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Board Game Assistant\n",
    "The following code blocks build two different variations on a RAG application using Gemini to answer questions about board game rules. The goal is to provide a simple interface for asking questions about rules, so that no one has to stop play to dig through the game manual. Both applications cite the manual page number from which the information was gathered, so that further research can be done quickly if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from google import genai\n",
    "from typing_extensions import List, TypedDict\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Assessment: Simple LLM ping without RAG\n",
    "First, let's check how well a basic implementation of Gemini can answer our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Puerto Rico, a production building is any building that produces goods when the **Producer** role is selected. These are the buildings that allow you to generate resources to then be traded or shipped for victory points.\n",
      "\n",
      "Here's a breakdown of what counts as a production building:\n",
      "\n",
      "*   **Indigo Plant:** Produces Indigo\n",
      "*   **Sugar Mill:** Produces Sugar\n",
      "*   **Corn Mill:** Produces Corn\n",
      "*   **Coffee Roaster:** Produces Coffee\n",
      "*   **Tobacco Storage:** Produces Tobacco\n",
      "\n",
      "**Key Considerations:**\n",
      "\n",
      "*   **Size:** The size of the building (Small, Large) doesn't affect whether it's a production building.\n",
      "*   **Not Included:** Buildings like the Guild Hall, Customs House, or University, even though they provide victory points or other benefits, are **not** production buildings because they don't directly produce goods.\n",
      "*   **Only the building itself:** Buildings which improve the production of a production building, such as the Sugar Mill adding a barrel, are not production buildings themselves.\n",
      "\n",
      "In summary, if a building produces Indigo, Sugar, Corn, Coffee, or Tobacco when the Producer role is active, it's a production building.\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "q = \"What counts as a production building in the game Puerto Rico?\"\n",
    "response = client.models.generate_content(model=\"gemini-2.0-flash\", contents=q)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information above is mostly correct, though the LLM is hallucinating about corn mills. It is also providing more information than is really needed to answer the question (e.g. the 'Key Considerations' section). There is a case to be made for trying RAG to get more reliable and relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Vector Store\n",
    "The code block below builds a vector store containing fragments from four different game manual pdfs. Alternatively, the pre-made vector store can be loaded directly using the next code block.  \n",
    "\n",
    "Here, pdfs are loaded page-by-page, and extracted langchain documents cannot span pages. This was done to make it easy for the application to reference the specific manual page used to generate the response. This behavior can be changed, allowing the model to read text chunks that span multiple pdf pages, by adding the mode='single' argument to the PyPDFLoader function.  \n",
    "\n",
    "A chunk size of 600 and overlap of 100 seems to work well for most manuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create vector store\n",
    "google_embedding = GoogleGenerativeAIEmbeddings(google_api_key=os.getenv(\"GEMINI_API_KEY\"), model=\"models/embedding-001\")\n",
    "vector_store = InMemoryVectorStore(embedding=google_embedding)\n",
    "\n",
    "## Load manuals into vector store \n",
    "manual_list = [(\"Everdell\",\"everdell-rulebook.pdf\"),\n",
    "               (\"Mysterium\",\"mysterium-rulebook.pdf\"),\n",
    "               (\"Puerto Rico\", \"Puerto-Rico-Deluxe-Rules.pdf\"),\n",
    "               (\"Ticket to Ride Europe\", \"ticket_to_ride_europe.pdf\")]\n",
    "path = \"./game_manuals/\" #path to directory containing game manuals\n",
    "\n",
    "for game in manual_list:\n",
    "    pdf_name = path+game[1]\n",
    "    game_name = game[0]\n",
    "    try:\n",
    "        #load text from pdf\n",
    "        loader = PyPDFLoader(pdf_name) #mode='single' to not separate by pages\n",
    "        pages = []\n",
    "        docs_lazy = loader.lazy_load()\n",
    "        for doc in docs_lazy:\n",
    "            doc.metadata['game'] = game_name\n",
    "            pages.append(doc)\n",
    "\n",
    "        ##Split text into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \", \".\"],\n",
    "                                                    chunk_size=600,\n",
    "                                                    chunk_overlap=100,\n",
    "                                                    add_start_index=True)\n",
    "        split_text = text_splitter.split_documents(pages)\n",
    "\n",
    "        ##Encode chunks and add to vector store\n",
    "        doc_ids = vector_store.add_documents(split_text)\n",
    "    \n",
    "    except:\n",
    "        print(\"Issue adding info from %s\"%pdf_name)\n",
    "        continue\n",
    "\n",
    "## Save vector store to json file\n",
    "vector_store.dump(\"board_game_vector_store.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load vector store from file\n",
    "google_embedding = GoogleGenerativeAIEmbeddings(google_api_key=os.getenv(\"GEMINI_API_KEY\"), model=\"models/embedding-001\")\n",
    "vector_store = InMemoryVectorStore(embedding=google_embedding).load(\"board_game_vector_store.json\", google_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG application version 1\n",
    "This RAG implementation returns the LLM's answer to the query along with a list of the documents retrieved from the similarity search of the vector store. Compared to version 2 of the application, it produces clearer instructions but cannot filter out irrelevant retrieved documents from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, production buildings are buildings that are required, along with plantations, to produce certain goods. These buildings are:\n",
      "\n",
      "*   Indigo processing plants (produce indigo dye - blue goods)\n",
      "*   Sugar mills (process sugar cane into sugar - white goods)\n",
      "*   Tobacco storage (shred tobacco leaves into tobacco - light brown goods)\n",
      "*   Coffee roasters (roast coffee beans into coffee - dark brown goods)\n",
      "\n",
      "There is no production building for corn.\n",
      "\n",
      "\n",
      "Referenced manual sections:\n",
      "\n",
      "Page 8:\n",
      "them. Of course, the player must also have sufficient occupied plantations of the \n",
      "appropriate kind to produce the raw materials needed to produce the goods in the \n",
      "production buildings.\n",
      "The buildings\n",
      "A player may build only\n",
      "one of each building.\n",
      "Only occupied  buildings have\n",
      "any use or value (except for VP\n",
      "value at game end) .\n",
      "The production buildings\n",
      "For corn there is no\n",
      "production building!\n",
      "The circles on the production\n",
      "buildings indicate the maximum\n",
      "number of goods the building can\n",
      "produce.\n",
      "Example\n",
      "The player produces\n",
      "the following goods:\n",
      "      2 corn goods\n",
      "(as the 3rd corn plantation\n",
      "\n",
      "\n",
      "Page 8:\n",
      "building cost has no further role in the game.\n",
      "The production buildings (blue, white, light and dark brown)\n",
      "The production buildings are required, together with the plantations, for the\n",
      "production of certain goods:\n",
      "- In the indigo processing plants, the indigo plants are processed to produce\n",
      "indigo dye (blue goods).\n",
      "- In the sugar mills, sugar cane is processed into sugar (white goods).\n",
      "- In the tobacco storage, the tobacco leaves are shredded into tobacco (light brown \n",
      "goods).\n",
      "- In the coffee roasters, the coffee beans are roasted into coffee (dark brown \n",
      "goods).\n",
      "\n",
      "\n",
      "Page 5:\n",
      "small warehouse large warehouse wharf\n",
      "construction hut large market harbor\n",
      "hacienda office university\n",
      "small market hospice  factory\n",
      "small sugar mill large sugar mill coffee roaster\n",
      "small indigo plant large indigo plant tobacco storage guild hall\n",
      "residence\n",
      "fortress\n",
      "customs house\n",
      "city hall\n",
      "PR-BanktableauUS.indd   1 12/27/18   4:47 AM\n",
      "5\n",
      "The builder’s privilege reduction is in addition to the quarry reduction, but\n",
      "the cost of a building may not be reduced below 0 doubloons. A player with 3\n",
      "occupied quarries pays the following costs: construction hut: 1 doubloon; office: 3\n",
      "\n",
      "\n",
      "Page 11:\n",
      "11\n",
      "The large buildings\n",
      "The following five large buildings occur just once each in the game. Each needs\n",
      "two adjacent city spaces to be built, but it counts as one building.\n",
      "Note: when, in these rules, “large building” is mentioned, it is the following five\n",
      "buildings that are described!\n",
      "Guild hall \n",
      "The owner of the occupied guild hall earns, at game end, an additional 1 VP\n",
      "for each small production building (occupied or unoccupied) in their city (=\n",
      "small indigo plant and small sugar mill), and an additional 2 VP for each large\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Build RAG state workflow, following https://python.langchain.com/docs/tutorials/rag\n",
    "\n",
    "#Prompt engineering\n",
    "prompt_text = \"You are an assistant for looking up board game rules. Use the following pieces of retrieved context from the board game manual, in addition to your own knowledge, to answer the question.\\n\" \\\n",
    "\"Game: {game}\\n\"\\\n",
    "\"Question: {question}\\n\" \\\n",
    "\"Context: {context}\"\n",
    "prompt = PromptTemplate.from_template(prompt_text)\n",
    "\n",
    "#Initiate LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "#Define application state\n",
    "class State(TypedDict):\n",
    "    game: str\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "#Retrieval step\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"], filter=lambda doc: doc.metadata.get(\"game\")==state[\"game\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "#Generation step\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"game\": state[\"game\"], \"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return({\"answer\":response.content})\n",
    "\n",
    "#Build graph\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "#Run test question and print response\n",
    "response = graph.invoke({\"question\":\"What counts as a production building?\", \"game\":\"Puerto Rico\"})\n",
    "print(response[\"answer\"])\n",
    "print(\"\\n\\nReferenced manual sections:\\n\")\n",
    "for doc in response[\"context\"]:\n",
    "    print(\"Page %s:\\n%s\\n\\n\"%(doc.metadata['page_label'], doc.page_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application produces a correct and well-formatted answer to the question without including irrelevant information. It lists the four types of production buildings and notes that there is no production building for the fifth resource, corn. However, it returns all documents gathered through a similarity search of the vector database, rather than just the most relevant document, and it does not format the retrieved document text very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG application version 2\n",
    "This implementation uses the with_structured_output tool to produce the specific manual snippet used to create the response. Responses tend to be shorter and less creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production buildings are blue, white, light, and dark brown buildings that are required, together with the plantations, for the production of certain goods. These buildings are the indigo processing plants, sugar mills, tobacco storage, and coffee roasters.\n",
      "\n",
      "Citations:\n",
      "page_num=8 quote='The production buildings (blue, white, light and dark brown)\\nThe production buildings are required, together with the plantations, for the\\nproduction of certain goods:\\n- In the indigo processing plants, the indigo plants are processed to produce\\nindigo dye (blue goods).\\n- In the sugar mills, sugar cane is processed into sugar (white goods).\\n- In the tobacco storage, the tobacco leaves are shredded into tobacco (light brown \\ngoods).\\n- In the coffee roasters, the coffee beans are roasted into coffee (dark brown \\ngoods).'\n"
     ]
    }
   ],
   "source": [
    "##Build RAG state workflow, following https://python.langchain.com/docs/how_to/qa_citations/\n",
    "\n",
    "#Prompt engineering\n",
    "prompt_text = \"You are an assistant for looking up board game rules. Use the following pieces of retrieved context from the board game manual, in addition to your own knowledge, to answer the question.\\n\" \\\n",
    "\"Game: {game}\\n\"\\\n",
    "\"Question: {question}\\n\" \\\n",
    "\"Context: {context}\"\n",
    "prompt = PromptTemplate.from_template(prompt_text)\n",
    "\n",
    "#Initialize LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "#Function to include pdf page labels with retrieved document contents\n",
    "def format_docs_with_page(docs: List[Document]) -> str:\n",
    "    formatted = [\n",
    "        f\"Page Number: {doc.metadata['page_label']}\\nManual Snippet: {doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ]\n",
    "    return \"\\n\\n\" + \"\\n\\n\".join(formatted)\n",
    "\n",
    "#Define quoted answer and citation classes\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    page_num: int = Field(\n",
    "        ...,\n",
    "        description=\"The page number of the SPECIFIC manual snippet which justifies the answer.\",\n",
    "    )\n",
    "    quote: str = Field(\n",
    "        ...,\n",
    "        description=\"The VERBATIM quote from the specified manual snippet that justifies the answer.\",\n",
    "    )\n",
    "\n",
    "class QuotedAnswer(BaseModel):\n",
    "    \"\"\"Answer the player's question based on the retrieved context from the game manual as well as your knowledge, and cite the relevant retrieved manual snippet.\"\"\"\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"The answer to the user question, based on the retrieved context and your knowledge.\",\n",
    "    )\n",
    "    citations: List[Citation] = Field(\n",
    "        ..., description=\"Citations from the given context that justify the answer.\"\n",
    "    )\n",
    "\n",
    "#Define application state\n",
    "class State(TypedDict):\n",
    "    game: str\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: QuotedAnswer\n",
    "\n",
    "#Retrieval step\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"], filter=lambda doc: doc.metadata.get(\"game\")==state[\"game\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "#Generation step\n",
    "def generate(state: State):\n",
    "    formatted_docs = format_docs_with_page(state[\"context\"])\n",
    "    messages = prompt.invoke({\"game\": state[\"game\"], \"question\": state[\"question\"], \"context\": formatted_docs})\n",
    "    structured_llm = llm.with_structured_output(QuotedAnswer)\n",
    "    response = structured_llm.invoke(messages)\n",
    "    return({\"answer\":response})\n",
    "\n",
    "#Build graph\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "#Run test question and print response\n",
    "response = graph.invoke({\"question\":\"What counts as a production building?\", \"game\":\"Puerto Rico\"})\n",
    "#print(response[\"answer\"])\n",
    "print(response['answer'].answer)\n",
    "print(\"\\nCitations:\")\n",
    "for cite in response['answer'].citations:\n",
    "    print(cite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application produces a correct and succinct answer to the question, with verbiage closely resembling that of the manual. It only cites the relevant documents retrieved from the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "RAG is helpful in generating more relevant answers to questions about board game rules. Both RAG implementations implemented here have benefits and drawbacks. Both are available as python scripts elsewhere in this repository for interactive use on the command line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
